{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c482fe3-5884-43a6-a2fa-2c3f2a6502a6",
   "metadata": {},
   "source": [
    "# Fine-Tuning de Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d0f7-d132-4e6e-ae2e-b9837c3b1e95",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de5505-866b-419d-93c8-d2dd36cfc11d",
   "metadata": {},
   "source": [
    "Veuillez utiliser ce notebook après que le notebook mistral_utilisation ait terminé son exécution avec succès.\n",
    "\n",
    "Dans ce notebook, nous allons effectuer le fine-tuning de Mistral-7B (https://huggingface.co/mistralai/Mistral-7B-v0.3) à partir de l'API Transformers d'Hugging Face (utilisant LoRa). Les étapes suivantes seront suivies :\n",
    "\n",
    "1. Installation des packages.\n",
    "2. Importation des bibliothèques nécessaires.\n",
    "3. Récupération des données.\n",
    "4. Prétraitement des données.\n",
    "5. Récupération du modèle sur le disque.\n",
    "6. Entraînement du modèle à partir des données.\n",
    "7. Utilisation du modèle entraîné sur un cas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ba7f-ec69-46a4-8509-7a641ab8a9da",
   "metadata": {},
   "source": [
    "## Installation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd8cee-c1ee-41c6-8425-0ede6a975da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153b6e9-89e6-481f-9a5e-4018a1ef4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b82e5-4fcc-4ec9-9e58-569344b18274",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f7234-1915-46ee-9102-9fa6606db872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour télécharger les données\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pour récupérer Mistral-7B, le tokenizer associé et ...\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Pour utiliser PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f6e74-a06e-4c7f-a18e-a49344ed79c3",
   "metadata": {},
   "source": [
    "## Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcbd8af-a539-4ad5-9553-6d9ed4e8e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8e51a-861f-4a5f-b2ca-e361bafe95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b24eb-a768-40fb-92d6-5aa7f036a58d",
   "metadata": {},
   "source": [
    "## Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a108d5e-eff2-4f33-8060-50ec666821b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2442ea-6cfc-43df-a455-0bcdd71bb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd401b-22c3-44e0-8fdb-1215e08a1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000)) # TODO : Rajouter la méthode shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c47a5b-0378-45eb-ad46-5e40c7eb7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200)) # TODO : Rajouter la méthode shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80739a48-a6ed-4be3-a8e0-e0cafceee09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde1c04-ed62-4d88-af5c-ba221494efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'une donnée (ici une donnée d'entraînement)\n",
    "print(instruct_tune_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e1ef0-12f4-44a6-874d-3656e9378364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de prompt d'une donnée originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879ed18-790d-4be7-88d3-72bf068e6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de response d'une donnée originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f35996-5b5e-44d3-89f0-a797aa3108ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de source d'une donnée originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86791009-bf09-4a9f-8ae1-a5fc5b284ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    \"\"\"\n",
    "    Modifie la donnée d'entrée pour correspondre au format attendu par Mistral-7B et à la tâche en question.\n",
    "\n",
    "    Paramètres:\n",
    "    sample (dict): La donnée d'entrée.\n",
    "\n",
    "    Retours:\n",
    "    str: La modification sous le bon format attendu par Mistral-7B et à la tâche en question.\n",
    "\n",
    "    Exemple:\n",
    "    >>> print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))\n",
    "    \n",
    "    <s>### Instruction:\n",
    "    Use the provided input to create a response to the prompt question.\n",
    "    \n",
    "    ### Input:\n",
    "    Can I find information about SALOME platform ?\n",
    "    \n",
    "    ### Response:\n",
    "    </s>\n",
    "    \"\"\"\n",
    "    \n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create a response to the prompt question.\"\n",
    "    response = sample[\"response\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    input = sample[\"prompt\"]\n",
    "    eos_token = \"</s>\"\n",
    "    \n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"### Instruction:\"\n",
    "    full_prompt += \"\\n\" + system_message\n",
    "    full_prompt += \"\\n\\n### Input:\"\n",
    "    full_prompt += \"\\n\" + input\n",
    "    full_prompt += \"\\n\\n### Response:\"\n",
    "    full_prompt += \"\\n\" + response\n",
    "    full_prompt += eos_token\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11856fb-024f-40ae-aeb2-23fa6c33dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la doc de la fonction create_prompt\n",
    "print(create_prompt.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d6219-debe-44e2-92d7-a6ac58ca8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation de la fonction create_prompt\n",
    "print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb331ad-8f73-441b-a1a4-493b34264c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de la donnée finale, i.e. après passage dans la fonction create_prompt\n",
    "print(create_prompt(instruct_tune_dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8c108-9f5f-4ec7-9992-71cb1bb7369e",
   "metadata": {},
   "source": [
    "## Récupération du modèle Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f146-52af-4564-be67-0a2a6879dffb",
   "metadata": {},
   "source": [
    "Dans cette section, on télécharge/récupère le modèle de base Mistral-7B, se trouvant sur le disque, et le tokenizer associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd619abd-7725-4c45-a638-4823760fab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: À étudier\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b693b1c-db2a-4f10-8321-93bebbe0a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification du répertoire de sauvegarde du modèle\n",
    "save_directory_model = \"../models/mistral7b_not_fine-tune\"\n",
    "\n",
    "# Spécification du répertoire de sauvegarde du tokenizer\n",
    "save_directory_tokenizer = \"../models/mistral7b_tokenizer_not_fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834b11c-51f4-45e0-a3e4-d30e8d9f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle sauvegardé\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory_model,\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f1e95-c2ad-4fa8-bf62-ee466d5acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du tokenizer associé\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory_tokenizer)\n",
    "\n",
    "# Problème: ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.\n",
    "# Solution: pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa0fdb-0d47-412e-97f6-151b964d5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: À étudier\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847286a-5d6a-476c-a68d-0db7bd94e9e7",
   "metadata": {},
   "source": [
    "## Fine-tuning du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387cf21-b0cf-4542-880a-c9dbaa390902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d492a4-49cb-400c-ac43-008484c1ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a59ab-cb90-443d-99cb-ef58259704da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"mistral_instruct_generation\",\n",
    "    #num_train_epochs=5,00:02.0\n",
    "    max_steps =5, # comment out this line if you want to train in epochs\n",
    "    per_device_train_batch_size = 2,\n",
    "    warmup_steps = 0,\n",
    "    logging_steps=1,\n",
    "    #save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=2, # comment out this line if you want to evaluate at the end of each epoch\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    per_gpu_train_batch_size=1,\n",
    "    lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309a2d7-572e-4fb5-8f58-2d3eb641a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 128\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36899bd8-4e2d-4d45-ad3c-25b0bdfe1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a202f3-b299-4b5f-a7f9-5a74296014c9",
   "metadata": {},
   "source": [
    "## Prédiction à partir du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff223d-232a-4bea-b9b7-4a69fb075d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1c291-f5a4-4a32-8c21-1dea29a5503b",
   "metadata": {},
   "source": [
    "Exemple d'utilisation de la fonction `generate_response` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd47e0-3c9a-4ab5-ac47-aa852152fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "#prompt = \"<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nI think it depends a little on the individual, but there are a number of steps you’ll need to take.  First, you’ll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You’ll also need to complete a residency program.  Once you have your education, you’ll need to be licensed.  And finally, you’ll need to establish a practice.\\n\\n### Response:\"\n",
    "#prompt = create_prompt()\n",
    "prompt = \"<s>### Instruction:\\nUse the provided input to create a response.\\n\\n### Input:\\nCan I find information about SALOME platform ?\\n\\n### Response:</s>\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abde696-8e5b-44ce-8b0a-c3bb4bf4948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réponse prédite par le modèle\n",
    "print(generate_response(prompt, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72991509-725d-4093-adea-3b5333009074",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdccb16-52da-4927-adf4-84f952865fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification du répertoire de sauvegarde\n",
    "save_directory = \"../models/mistral7b_fine-tune\"\n",
    "\n",
    "# Sauvegarde du modèle fine-tuné\n",
    "model.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stage-cea-chatbot)",
   "language": "python",
   "name": "stage-cea-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
