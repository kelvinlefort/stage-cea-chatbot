{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611efd7b-d5c6-4f18-a8a3-aee440a7d620",
   "metadata": {},
   "source": [
    "# Comparaison de la taille du modèle Mistral-7B-v0.1 sauvegardé sur le disque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10ec45-5266-44f1-b05a-8c70a845e6e9",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons comparer la taille du modèle **Mistral-7B-v0.1** sauvegardé sur le disque, modèle pré-entraîné et modèle fine-tuné. On utiisera la **quantization** pour l'utilisation du modèle et la méthode **LoRA** pour le fine-tuning. Les étapes suivantes seront suivies :\n",
    "\n",
    "1. Récupération du modèle Mistral-7B-v0.1.\n",
    "2. Sauvegarde du modèle Mistral-7B-v0.1 sur le disque.\n",
    "3. Fine-tuning du modèle Mistral-7B-v0.1.\n",
    "4. Sauvegarde du modèle Mistral-7B-v0.1 fine-tuné sur le disque.\n",
    "5. Comparaison des tailles.\n",
    "\n",
    "Note : Les éléments suivants sont requis :\n",
    "- Compte Hugging Face\n",
    "- Accéder au modèle Mistral-7B v0.1 (https://huggingface.co/mistralai/Mistral-7B-v0.1) sur Hugging Face et accepter les termes et conditions\n",
    "- Créer un \"READ\" token à partir de vos paramètres sur Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2106a6-a159-4e4b-9415-23f0b4d794a3",
   "metadata": {},
   "source": [
    "## Récupération du modèle Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcdc40-cd43-45b8-9a22-a4d83dd29066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900b2ce-4ad4-4dde-bdeb-160a5b888234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Le GPU est utilisé ici par défaut, pas besoin de charger le module CUDA\n",
    "# - Ne pas lancer ce bout de codes plusieurs fois, sinon : ValueError: Some modules are dispatched on the CPU or the disk...\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c32aa-5599-4667-94dd-1e6b96f834b9",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle Mistral-7B-v0.1 sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042682c-2844-45f7-95d3-4bc1846f25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification du répertoire de sauvegarde\n",
    "save_directory = \"../models/mistral-7b-v0.1\"\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9e38b-b649-4fba-a6eb-383e594d6481",
   "metadata": {},
   "source": [
    "## Fine-tuning du modèle Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9352b-8724-4834-bb39-a635f4be1606",
   "metadata": {},
   "source": [
    "Pour le fine-tuning du modèle Mistral-7B-v0.1, consulter le notebook `mistral_finetune_own_data.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea318ee1-6ab1-4189-be46-8d95213c193e",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle Mistral-7B-v0.1 fine-tuné sur le disque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa330ae-9cec-45ce-bdf5-cc8c1cf85808",
   "metadata": {},
   "source": [
    "Pour la sauvegarde du modèle Mistral-7B-v0.1 fine-tuné sur le disque, consulter le notebook `mistral_finetune_own_data.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6e756-48e9-481f-9480-5f1360658c3e",
   "metadata": {},
   "source": [
    "## Comparaison des tailles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550ecb8-ca42-4d75-bf54-c0afd248cbdf",
   "metadata": {},
   "source": [
    "La sauvegarde d'un modèle est réalisée avec deux types de fichiers :\n",
    "\n",
    "- un (ou des) fichier(s) de configuration `.json`.\n",
    "- un fichier du modèle `.safetensors`.\n",
    "\n",
    "La majorité de l'information du modèle est contenue dans le fichier `.safetensors`, étant donné que le(s) fichier(s) `.json` sont simplement là pour la configuration.\n",
    "\n",
    "On observe alors la taille des fichiers `.safetensors` :\n",
    "\n",
    "- modèle pré-entraîné : 4 125 687 623 octets\n",
    "- modèle fine-tuné : 864 513 616 octets\n",
    "\n",
    "Le rapport de taille vaut :\n",
    "\n",
    "$$\n",
    "\\frac{864 513 616}{4 125 687 623} = 0.2095\n",
    "$$\n",
    "\n",
    "Le modèle fine-tuné occupe donc 80 % d'emplacement mémoire en moins que le modèle pré-entaîné !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d841565-aeed-4787-ad96-ce149d3b11a6",
   "metadata": {},
   "source": [
    "TODO: \n",
    "CPU- GPU pour quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stage-cea-chatbot)",
   "language": "python",
   "name": "stage-cea-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
